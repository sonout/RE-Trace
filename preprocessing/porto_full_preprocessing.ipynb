{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b85ed9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import yaml\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import sys\n",
    "from tqdm import tqdm\n",
    "\n",
    "sys.path.append('../src')\n",
    "\n",
    "import threads as threads\n",
    "from utils import create_modified_set, pkl2h5, read_train_file, pkl2h5_wo_time\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "data_path = '/home/schestakov/data/re-identification/porto/'\n",
    "#data_save_path = '/home/schestakov/data/re-identification/porto/db_exp2'\n",
    "data_save_path = '/home/schestakov/projects/re-identification/data/porto/db'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c83e0b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Refer to porto_data_analysis.ipynb to see how to get from original data to list of trajectories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "504126d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of Trajectories\n",
    "# Trajectory: [[x1,y1,t1], [x2,y2,t2], ... , [xn,yn,tn]]\n",
    "traj_list = pickle.load(open(os.path.join(data_path, \"traj_list_notime.pkl\"), \"rb\"))\n",
    "\n",
    "n_samples  = len(traj_list)\n",
    "n_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4af354e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove all trajectories with less than 10 points\n",
    "new_traj_list = []\n",
    "for t in traj_list:\n",
    "    l = len(t)\n",
    "    if l < 10:\n",
    "        new_traj_list.append(t)\n",
    "print(len(new_traj_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c9bd78",
   "metadata": {},
   "outputs": [],
   "source": [
    "traj_list = new_traj_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2660e539",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(traj_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d936f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split by 80%, 10%, 10%\n",
    "train = traj_list[:int(0.8*n_samples)]\n",
    "val = traj_list[int(0.8*n_samples):int(0.9*n_samples)]\n",
    "test = traj_list[int(0.9*n_samples):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb3390d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train))\n",
    "print(len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "130ea4af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save train, val, test\n",
    "with open(os.path.join(data_save_path, 'train.pkl'), \"wb\") as f:\n",
    "    pickle.dump(train, f)\n",
    "with open(os.path.join(data_save_path, 'val.pkl'), \"wb\") as f:\n",
    "    pickle.dump(val, f)\n",
    "with open(os.path.join(data_save_path, 'test.pkl'), \"wb\") as f:\n",
    "    pickle.dump(test, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac2dc16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load\n",
    "train = pickle.load(open(os.path.join(data_save_path, \"train.pkl\"), \"rb\"))\n",
    "val = pickle.load(open(os.path.join(data_save_path, \"val.pkl\"), \"rb\"))\n",
    "test = pickle.load(open(os.path.join(data_save_path, \"test.pkl\"), \"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f9f2751",
   "metadata": {},
   "source": [
    "### Apply Threads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa55d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "###### Add Threads to test #######\n",
    "\n",
    "# split test into own and other set\n",
    "# For each thread: \n",
    "#     - attack on own set\n",
    "#     - attack on other set (add to not evaluating finding attack structures in trace)\n",
    "#     - add labels: true if attack on own set, else false\n",
    "\n",
    "#own_share = 0.1\n",
    "\n",
    "#own_set = test[:int(own_share*len(test))]\n",
    "#other_set = test[int(own_share*len(test)):]\n",
    "\n",
    "own_size = 1000\n",
    "own_set = test[:own_size]\n",
    "rest = test[own_size:]\n",
    "\n",
    "\n",
    "print(len(own_set))\n",
    "print(len(rest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae8e998",
   "metadata": {},
   "outputs": [],
   "source": [
    "attacks_dict = {\n",
    "    'Multi:RAP+DS+GWN': [threads.resample_along_path, threads.downsample, threads.add_signal_noise],\n",
    "    'GWN:white_noise': threads.add_white_noise,\n",
    "    'SNR:signal_to_noise': threads.add_signal_noise,\n",
    "    'OSNR:outliers_with_snr': threads.add_outliers_with_signal_to_noise_ratio,\n",
    "    'RIP:rounding': threads.remove_non_significant_bits, \n",
    "    'DS:downsample': threads.downsample,\n",
    "    'RRPP:replace_random_points_with_path': threads.replace_random_points_with_path,\n",
    "    'RNSPP:replace_non_skeleton_points_with_path': threads.replace_non_skeleton_points_with_path,\n",
    "    'RAP:resample_along_path': threads.resample_along_path,\n",
    "    'C:cropping': threads.cropping,\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d33713ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "other_set = rest[:own_size]\n",
    "\n",
    "\n",
    "total_set = []\n",
    "total_set_labels = []\n",
    "total_set_description = []\n",
    "\n",
    "\n",
    "for attack_name, attack_function in attacks_dict.items():\n",
    "\n",
    "    # Create own set\n",
    "    mod_own, own_labels, own_attack_descr = create_modified_set(own_set, attack_name, attack_function, is_own_set = True)\n",
    "    mod_other, other_labels, other_attack_descr = create_modified_set(other_set, attack_name, attack_function, is_own_set = False)\n",
    "    \n",
    "    # Add to total sets\n",
    "    total_set = total_set + mod_own + mod_other\n",
    "    total_set_labels = total_set_labels + own_labels + other_labels\n",
    "    total_set_description = total_set_description + own_attack_descr + other_attack_descr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b5a7688",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle all sets\n",
    "zipped_set = list(zip(total_set, total_set_labels, total_set_description))\n",
    "random.shuffle(zipped_set)\n",
    "total_set, total_set_labels, total_set_description = zip(*zipped_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b769e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save own_set, total_set, total_set_description\n",
    "with open(os.path.join(data_save_path, 'own_set.pkl'), \"wb\") as f:\n",
    "    pickle.dump(own_set, f)\n",
    "with open(os.path.join(data_save_path, 'total_set.pkl'), \"wb\") as f:\n",
    "    pickle.dump(total_set, f)\n",
    "with open(os.path.join(data_save_path, 'total_set_labels.pkl'), \"wb\") as f:\n",
    "    pickle.dump(total_set_labels, f)\n",
    "with open(os.path.join(data_save_path, 'total_set_description.pkl'), \"wb\") as f:\n",
    "    pickle.dump(total_set_description, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2bff598",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fe26111",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "555919d3",
   "metadata": {},
   "source": [
    "### Preprecess further for DL models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3520e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "julia_path = \"/home/schestakov/downloads/julia/julia-1.8.5/bin/julia\"\n",
    "hyper_param_path = \"./hyper-parameters.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c4deac",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_save_path = os.path.join(data_save_path, \"dl_models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "913bec02",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Training ####\n",
    "\n",
    "# IMPORTANT: \n",
    "# 1. Check that we save the trajectories with lat,lon in this order: [-8.619489, 41.175018]\n",
    "#    Therefore, if traj_list has it like [41.148009, -8.619777] we need to swap the order (swap_lon_lat = True)\n",
    "#              Else, we can leave it as it is (swap_lon_lat = False)\n",
    "# 2. Check if we have time in the trajectories -> (remove_time = False or True)\n",
    "\n",
    "cityname = \"porto\"\n",
    "# for training\n",
    "pkl2h5(train, data_save_path, \"train.h5\", swap_lon_lat = False, remove_time = False)\n",
    "pkl2h5(val, data_save_path, \"val.h5\", swap_lon_lat = False, remove_time = False)\n",
    "# To build the spatial region we create a city.h5 file with all trajectories\n",
    "# We use from last iteration, as it will be the same\n",
    "#pkl2h5(traj_list, data_save_path, f\"{cityname}.h5\", swap_lon_lat = False, remove_time = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0978e328",
   "metadata": {},
   "outputs": [],
   "source": [
    "from subprocess import call\n",
    "# This function creates preprocesses train.h5 and val.h5 for training. Output is train.src, train.trg, val.src, val.trg\n",
    "call([julia_path, \"preprocess.jl\", \"--datapath\", data_save_path, \"--parampath\", hyper_param_path])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "302752aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For our model we convert into numpy as save as .npz\n",
    "max_len = 100\n",
    "train_src, train_src_len = read_train_file(os.path.join(data_save_path, \"train.src\"), max_len)\n",
    "train_trg, train_trg_len = read_train_file(os.path.join(data_save_path, \"train.trg\"), max_len)\n",
    "val_src, val_src_len = read_train_file(os.path.join(data_save_path, \"val.src\"), max_len)\n",
    "val_trg, val_trg_len = read_train_file(os.path.join(data_save_path, \"val.trg\"), max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f8237f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_src = np.array(train_src)\n",
    "train_trg = np.array(train_trg)\n",
    "val_src = np.array(val_src)\n",
    "val_trg = np.array(val_trg)\n",
    "\n",
    "train_src_len = np.array(train_src_len)\n",
    "train_trg_len = np.array(train_trg_len)\n",
    "val_src_len = np.array(val_src_len)\n",
    "val_trg_len = np.array(val_trg_len)\n",
    "\n",
    "print(train_src.shape)\n",
    "print(train_trg.shape)\n",
    "print(val_src.shape)\n",
    "print(val_trg.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c942a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Also we dont use validation set so stack it with training set\n",
    "train_src = np.concatenate((train_src,val_src), axis=0)\n",
    "train_trg = np.concatenate((train_trg,val_trg), axis=0)\n",
    "train_src_len = np.concatenate((train_src_len,val_src_len), axis=0)\n",
    "train_trg_len = np.concatenate((train_trg_len,val_trg_len), axis=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd07c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save compressed\n",
    "np.savez_compressed(os.path.join(data_save_path,\"train.npz\"), src=train_src, trg=train_trg, src_len=train_src_len, trg_len=train_trg_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c440f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading example:\n",
    "loaded = np.load(os.path.join(data_save_path,\"train.npz\"))\n",
    "src= loaded['src']\n",
    "trg= loaded['trg']\n",
    "src_len= loaded['src_len']\n",
    "trg_len= loaded['trg_len']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81bcdfbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Evaluation ####\n",
    "\n",
    "\n",
    "# Format of .pkl files\n",
    "# List of Trajectories: [T1, T2, ... , Tn]\n",
    "# Trajectory: T = [[x1,y1,t1], [x2,y2,t2], ... , [xn,yn,tn]]\n",
    "\n",
    "load = False\n",
    "if load:\n",
    "    own_set = pickle.load(open(os.path.join(data_save_path, \"own_set.pkl\"), \"rb\"))\n",
    "    total_set = pickle.load(open(os.path.join(data_save_path, \"total_set.pkl\"), \"rb\"))\n",
    "\n",
    "# We need to convert train, val, own_set, total_set\n",
    "print(f\"own: {len(own_set)} \\ntotal: {len(total_set)}\")\n",
    "\n",
    "# For evaluation\n",
    "pkl2h5(own_set, data_save_path, \"own.h5\", swap_lon_lat = False, remove_time = False)\n",
    "pkl2h5(total_set, data_save_path, \"total.h5\", swap_lon_lat = False, remove_time = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dac1282",
   "metadata": {},
   "outputs": [],
   "source": [
    "from subprocess import call\n",
    "# From .h5 files map to grid and safe as .t file\n",
    "filenames = [\"own\", \"total\"]\n",
    "for name in filenames:\n",
    "    call([julia_path, \"traj2gridseq.jl\", \"--datapath\", data_save_path, \"--filename\", name,  \"--parampath\", hyper_param_path])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "029fe090",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "traje",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
