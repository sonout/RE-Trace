{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b85ed9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import yaml\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import sys\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "sys.path.append('../src')\n",
    "\n",
    "import threads as threads\n",
    "from utils import create_attacked_sets, pkl2h5, read_train_file, pkl2h5_wo_time\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "data_path = '/home/schestakov/projects/re-identification/data/sf/db'\n",
    "data_save_path = '/home/schestakov/projects/re-identification/data/sf/db'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a765a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "504126d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "695675"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List of Trajectories\n",
    "# Trajectory: [[x1,y1,t1], [x2,y2,t2], ... , [xn,yn,tn]]\n",
    "traj_list = pickle.load(open(os.path.join(data_path, \"traj_list.pkl\"), \"rb\"))\n",
    "\n",
    "n_samples  = len(traj_list)\n",
    "n_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2660e539",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(traj_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3d936f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split by 80%, 10%, 10%\n",
    "train = traj_list[:int(0.8*n_samples)]\n",
    "val = traj_list[int(0.8*n_samples):int(0.9*n_samples)]\n",
    "test = traj_list[int(0.9*n_samples):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eb3390d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "556540\n",
      "69568\n"
     ]
    }
   ],
   "source": [
    "######## JUST FOR IMPLEMENTATION PHASE REDUCE SIZE #########\n",
    "print(len(train))\n",
    "#test = test[:10000]\n",
    "print(len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "130ea4af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save train, val, test\n",
    "with open(os.path.join(data_save_path, 'train.pkl'), \"wb\") as f:\n",
    "    pickle.dump(train, f)\n",
    "with open(os.path.join(data_save_path, 'val.pkl'), \"wb\") as f:\n",
    "    pickle.dump(val, f)\n",
    "with open(os.path.join(data_save_path, 'test.pkl'), \"wb\") as f:\n",
    "    pickle.dump(test, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f9f2751",
   "metadata": {},
   "source": [
    "### Apply Threads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "faa55d0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "68568\n"
     ]
    }
   ],
   "source": [
    "###### Add Threads to test #######\n",
    "\n",
    "# split test into own and other set\n",
    "# For each thread: \n",
    "#     - attack on own set\n",
    "#     - attack on other set (add to not evaluating finding attack structures in trace)\n",
    "#     - add labels: true if attack on own set, else false\n",
    "\n",
    "#own_share = 0.1\n",
    "\n",
    "#own_set = test[:int(own_share*len(test))]\n",
    "#other_set = test[int(own_share*len(test)):]\n",
    "\n",
    "own_size = 1000\n",
    "own_set = test[:own_size]\n",
    "other_set = test[own_size:]\n",
    "\n",
    "\n",
    "print(len(own_set))\n",
    "print(len(other_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3527988",
   "metadata": {},
   "outputs": [],
   "source": [
    "attacks_dict = {\n",
    "    'GWN:white_noise': threads.add_white_noise,\n",
    "    'SNR:signal_to_noise': threads.add_signal_noise,\n",
    "    'OSNR:outliers_with_snr': threads.add_outliers_with_signal_to_noise_ratio,\n",
    "    'RIP:rounding': threads.remove_non_significant_bits, \n",
    "    'DS:downsample': threads.downsample,\n",
    "    'RRPP:replace_random_points_with_path': threads.replace_random_points_with_path,\n",
    "    'RNSPP:replace_non_skeleton_points_with_path': threads.replace_non_skeleton_points_with_path,\n",
    "    'RAP:resample_along_path': threads.resample_along_path,\n",
    "    'C:cropping': threads.cropping,\n",
    "    'Multi:DS+GWN+C': [threads.add_white_noise, threads.downsample, threads.resample_along_path],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d33713ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_set = []\n",
    "total_set_labels = []\n",
    "total_set_description = []\n",
    "for attack_name, attack_function in attacks_dict.items():\n",
    "    attacked_traj, labels, attack_descr = create_attacked_sets(own_set, other_set, attack_name, attack_function)\n",
    "    \n",
    "    # Add to total sets\n",
    "    total_set = total_set + attacked_traj\n",
    "    total_set_labels = total_set_labels + labels\n",
    "    total_set_description = total_set_description + attack_descr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1b5a7688",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffel all sets\n",
    "zipped_set = list(zip(total_set, total_set_labels, total_set_description))\n",
    "random.shuffle(zipped_set)\n",
    "total_set, total_set_labels, total_set_description = zip(*zipped_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "55b769e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save own_set, total_set, total_set_description\n",
    "with open(os.path.join(data_save_path, 'own_set.pkl'), \"wb\") as f:\n",
    "    pickle.dump(own_set, f)\n",
    "with open(os.path.join(data_save_path, 'total_set.pkl'), \"wb\") as f:\n",
    "    pickle.dump(total_set, f)\n",
    "with open(os.path.join(data_save_path, 'total_set_labels.pkl'), \"wb\") as f:\n",
    "    pickle.dump(total_set_labels, f)\n",
    "with open(os.path.join(data_save_path, 'total_set_description.pkl'), \"wb\") as f:\n",
    "    pickle.dump(total_set_description, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2fe26111",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open all files\n",
    "with open(os.path.join(data_save_path, 'train.pkl'), \"rb\") as f:\n",
    "    train = pickle.load(f)\n",
    "with open(os.path.join(data_save_path, 'val.pkl'), \"rb\") as f:\n",
    "    val = pickle.load(f)\n",
    "with open(os.path.join(data_save_path, 'test.pkl'), \"rb\") as f:\n",
    "    test = pickle.load(f)\n",
    "with open(os.path.join(data_save_path, 'own_set.pkl'), \"rb\") as f:\n",
    "    own_set = pickle.load(f)\n",
    "with open(os.path.join(data_save_path, 'total_set.pkl'), \"rb\") as f:\n",
    "    total_set = pickle.load(f)\n",
    "with open(os.path.join(data_save_path, 'total_set_labels.pkl'), \"rb\") as f:\n",
    "    total_set_labels = pickle.load(f)\n",
    "with open(os.path.join(data_save_path, 'total_set_description.pkl'), \"rb\") as f:\n",
    "    total_set_description = pickle.load(f)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "555919d3",
   "metadata": {},
   "source": [
    "### Preprecess further for DL models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "765b4091",
   "metadata": {},
   "outputs": [],
   "source": [
    "julia_path = \"/home/schestakov/downloads/julia-1.8.5/bin/julia\"\n",
    "hyper_param_path = \"./hyper-parameters_sf.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "116b7bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_save_path = os.path.join(data_save_path, '50')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e859d9e0",
   "metadata": {},
   "source": [
    "#### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "913bec02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed writing 556540 to /home/schestakov/projects/re-identification/data/sf/db/50/train.h5\n",
      "Completed writing 69567 to /home/schestakov/projects/re-identification/data/sf/db/50/val.h5\n",
      "Completed writing 695675 to /home/schestakov/projects/re-identification/data/sf/db/50/sf.h5\n"
     ]
    }
   ],
   "source": [
    "##### Training ####\n",
    "\n",
    "cityname = \"sf\"\n",
    "# for training\n",
    "pkl2h5(train, data_save_path, \"train.h5\")\n",
    "pkl2h5(val, data_save_path, \"val.h5\")\n",
    "# To build the spatial region we create a city.h5 file with all trajectories\n",
    "pkl2h5(train+val+test, data_save_path, f\"{cityname}.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0978e328",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building spatial region with:\n",
      "        cityname=sf,\n",
      "        minlon=-122.5183,\n",
      "        minlat=37.085,\n",
      "        maxlon=-121.5927,\n",
      "        maxlat=38.3473,\n",
      "        xstep=50.0,\n",
      "        ystep=50.0,\n",
      "        minfreq=50\n",
      "Creating paramter file /home/schestakov/projects/re-identification/data/sf/db/50/sf-param-cell50\n",
      "Processed 100000 trips\n",
      "Processed 200000 trips\n",
      "Processed 300000 trips\n",
      "Processed 400000 trips\n",
      "Processed 500000 trips\n",
      "Processed 600000 trips\n",
      "*\n",
      "Cell count at max_num_hotcells:40000\n",
      "*\n",
      "138\n",
      "*\n",
      "Cell count at max_num_hotcells:40000 is 138\n",
      "Vocabulary size 40004 with cell size 50.0 (meters)\n",
      "Creating training and validation datasets...\n",
      "Opening H5 file at /home/schestakov/projects/re-identification/data/sf/db/50/train.h5\n",
      "Scaned 10000 trips...\n",
      "Scaned 20000 trips...\n",
      "Scaned 30000 trips...\n",
      "Scaned 40000 trips...\n",
      "Scaned 50000 trips...\n",
      "Scaned 60000 trips...\n",
      "Scaned 70000 trips...\n",
      "Scaned 80000 trips...\n",
      "Scaned 90000 trips...\n",
      "Scaned 100000 trips...\n",
      "Scaned 110000 trips...\n",
      "Scaned 120000 trips...\n",
      "Scaned 130000 trips...\n",
      "Scaned 140000 trips...\n",
      "Scaned 150000 trips...\n",
      "Scaned 160000 trips...\n",
      "Scaned 170000 trips...\n",
      "Scaned 180000 trips...\n",
      "Scaned 190000 trips...\n",
      "Scaned 200000 trips...\n",
      "Scaned 210000 trips...\n",
      "Scaned 220000 trips...\n",
      "Scaned 230000 trips...\n",
      "Scaned 240000 trips...\n",
      "Scaned 250000 trips...\n",
      "Scaned 260000 trips...\n",
      "Scaned 270000 trips...\n",
      "Scaned 280000 trips...\n",
      "Scaned 290000 trips...\n",
      "Scaned 300000 trips...\n",
      "Scaned 310000 trips...\n",
      "Scaned 320000 trips...\n",
      "Scaned 330000 trips...\n",
      "Scaned 340000 trips...\n",
      "Scaned 350000 trips...\n",
      "Scaned 360000 trips...\n",
      "Scaned 370000 trips...\n",
      "Scaned 380000 trips...\n",
      "Scaned 390000 trips...\n",
      "Scaned 400000 trips...\n",
      "Scaned 410000 trips...\n",
      "Scaned 420000 trips...\n",
      "Scaned 430000 trips...\n",
      "Scaned 440000 trips...\n",
      "Scaned 450000 trips...\n",
      "Scaned 460000 trips...\n",
      "Scaned 470000 trips...\n",
      "Scaned 480000 trips...\n",
      "Scaned 490000 trips...\n",
      "Scaned 500000 trips...\n",
      "Scaned 510000 trips...\n",
      "Scaned 520000 trips...\n",
      "Scaned 530000 trips...\n",
      "Scaned 540000 trips...\n",
      "Scaned 550000 trips...\n",
      "Opening H5 file at /home/schestakov/projects/re-identification/data/sf/db/50/val.h5\n",
      "Scaned 10000 trips...\n",
      "Scaned 20000 trips...\n",
      "Scaned 30000 trips...\n",
      "Scaned 40000 trips...\n",
      "Scaned 50000 trips...\n",
      "Scaned 60000 trips...\n",
      "Saved cell distance into /home/schestakov/projects/re-identification/data/sf/db/50/sf-vocab-dist-cell50.h5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from subprocess import call\n",
    "# This function creates preprocesses train.h5 and val.h5 for training. Output is train.src, train.trg, val.src, val.trg\n",
    "call([julia_path, \"preprocess.jl\", \"--datapath\", data_save_path, \"--parampath\", hyper_param_path])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "302752aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing file: /home/schestakov/projects/re-identification/data/sf/db/50/train.src\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11130800it [01:14, 149434.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing file: /home/schestakov/projects/re-identification/data/sf/db/50/train.trg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11130800it [01:29, 123764.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing file: /home/schestakov/projects/re-identification/data/sf/db/50/val.src\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1391340it [00:07, 176284.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing file: /home/schestakov/projects/re-identification/data/sf/db/50/val.trg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1391340it [00:08, 155375.16it/s]\n"
     ]
    }
   ],
   "source": [
    "# For our model we convert into numpy as save as .npz\n",
    "max_len = 100\n",
    "train_src, train_src_len = read_train_file(os.path.join(data_save_path, \"train.src\"), max_len)\n",
    "train_trg, train_trg_len = read_train_file(os.path.join(data_save_path, \"train.trg\"), max_len)\n",
    "val_src, val_src_len = read_train_file(os.path.join(data_save_path, \"val.src\"), max_len)\n",
    "val_trg, val_trg_len = read_train_file(os.path.join(data_save_path, \"val.trg\"), max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5f8237f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(556539, 19, 100)\n",
      "(556539, 19, 100)\n",
      "(69566, 19, 100)\n",
      "(69566, 19, 100)\n"
     ]
    }
   ],
   "source": [
    "train_src = np.array(train_src)\n",
    "train_trg = np.array(train_trg)\n",
    "val_src = np.array(val_src)\n",
    "val_trg = np.array(val_trg)\n",
    "\n",
    "train_src_len = np.array(train_src_len)\n",
    "train_trg_len = np.array(train_trg_len)\n",
    "val_src_len = np.array(val_src_len)\n",
    "val_trg_len = np.array(val_trg_len)\n",
    "\n",
    "print(train_src.shape)\n",
    "print(train_trg.shape)\n",
    "print(val_src.shape)\n",
    "print(val_trg.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "91c942a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Also we dont use validation set so stack it with training set\n",
    "train_src = np.concatenate((train_src,val_src), axis=0)\n",
    "train_trg = np.concatenate((train_trg,val_trg), axis=0)\n",
    "train_src_len = np.concatenate((train_src_len,val_src_len), axis=0)\n",
    "train_trg_len = np.concatenate((train_trg_len,val_trg_len), axis=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2cd07c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save compressed\n",
    "np.savez_compressed(os.path.join(data_save_path,\"train.npz\"), src=train_src, trg=train_trg, src_len=train_src_len, trg_len=train_trg_len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c440f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading example:\n",
    "loaded = np.load(os.path.join(data_save_path,\"train.npz\"))\n",
    "src= loaded['src']\n",
    "trg= loaded['trg']\n",
    "src_len= loaded['src_len']\n",
    "trg_len= loaded['trg_len']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85244206",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3e060120",
   "metadata": {},
   "source": [
    "#### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bb1962c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[37.78235000000001, -122.42520000000002]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "own_set[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "81bcdfbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "own: 1000 \n",
      "total: 20000\n",
      "Completed writing 1000 to /home/schestakov/projects/re-identification/data/sf/db/50/own.h5\n",
      "Completed writing 20000 to /home/schestakov/projects/re-identification/data/sf/db/50/total.h5\n"
     ]
    }
   ],
   "source": [
    "##### Evaluation ####\n",
    "\n",
    "\n",
    "# Format of .pkl files\n",
    "# List of Trajectories: [T1, T2, ... , Tn]\n",
    "# Trajectory: T = [[x1,y1,t1], [x2,y2,t2], ... , [xn,yn,tn]]\n",
    "\n",
    "load = False\n",
    "if load:\n",
    "    own_set = pickle.load(open(os.path.join(data_save_path, \"own_set.pkl\"), \"rb\"))\n",
    "    total_set = pickle.load(open(os.path.join(data_save_path, \"total_set.pkl\"), \"rb\"))\n",
    "\n",
    "\n",
    "# We need to convert train, val, own_set, total_set\n",
    "print(f\"own: {len(own_set)} \\ntotal: {len(total_set)}\")\n",
    "\n",
    "# For evaluation\n",
    "pkl2h5_wo_time(own_set, data_save_path, \"own.h5\")\n",
    "pkl2h5_wo_time(total_set, data_save_path, \"total.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3dac1282",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building spatial region with:\n",
      "        cityname=sf,\n",
      "        minlon=-122.5183,\n",
      "        minlat=37.085,\n",
      "        maxlon=-121.5927,\n",
      "        maxlat=38.3473,\n",
      "        xstep=50.0,\n",
      "        ystep=50.0,\n",
      "        minfreq=50\n",
      "Reading parameter file from /home/schestakov/projects/re-identification/data/sf/db/50/sf-param-cell50\n",
      "Loaded /home/schestakov/projects/re-identification/data/sf/db/50/sf-param-cell50 into region\n",
      "Building spatial region with:\n",
      "        cityname=sf,\n",
      "        minlon=-122.5183,\n",
      "        minlat=37.085,\n",
      "        maxlon=-121.5927,\n",
      "        maxlat=38.3473,\n",
      "        xstep=50.0,\n",
      "        ystep=50.0,\n",
      "        minfreq=50\n",
      "Reading parameter file from /home/schestakov/projects/re-identification/data/sf/db/50/sf-param-cell50\n",
      "Loaded /home/schestakov/projects/re-identification/data/sf/db/50/sf-param-cell50 into region\n"
     ]
    }
   ],
   "source": [
    "from subprocess import call\n",
    "# From .h5 files map to grid and safe as .t file\n",
    "filenames = ['own', 'total']\n",
    "for name in filenames:\n",
    "    call([julia_path, \"traj2gridseq.jl\", \"--datapath\", data_save_path, \"--filename\", name,  \"--parampath\", hyper_param_path])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "029fe090",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "3463fa7b26682fd29d4fd2c5c0de2091fc59b99f02dd841729d1b2b06fe27822"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
